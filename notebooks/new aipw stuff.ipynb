{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "path_to_project = os.path.abspath(os.path.join(os.getcwd(), '../'))    \n",
    "sys.path.insert(1, os.path.join(path_to_project))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from src.utils import one_hot\n",
    "from src.directory import data_dir, NHANES_preprocessed_filename\n",
    "from src.estimators import aipw_estimator, unadjusted_DM_estimator, ipw_estimator, t_learner, s_learner, x_learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load df\n",
    "NHANES_preprocessed_filepath = os.path.join(data_dir, NHANES_preprocessed_filename)\n",
    "df = pd.read_csv(NHANES_preprocessed_filepath, index_col='SEQN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# patients with complete data:  2548\n"
     ]
    }
   ],
   "source": [
    "df.dropna(how='any', inplace=True)\n",
    "df = df[df['age']>=35]\n",
    "df.rename(columns={'martial_status': 'marital_status'}, inplace=True)\n",
    "print('# patients with complete data: ', len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define relevant features\n",
    "# features\n",
    "z_col = 'ambient_light'\n",
    "t_col = 'sleep_deprivation'\n",
    "y_cols = ['HTN', 'DBP', 'SBP']\n",
    "\n",
    "# column types\n",
    "categorical_cols = ['physical_activity', 'ANTIDEPRESSANTS_ANXIOLYTICS', 'GLUCOCORTICOIDS', 'sleep_troubles',\n",
    "       'sleep_deprivation', 'diabetes', 'smoker', 'race_ethnicity', 'gender', 'health_insurance', 'marital_status','CVD', \n",
    "       'smoker_hx', 'HTN']\n",
    "numerical_cols = ['daily_sedentary', 'accelerometer', 'BMI', 'age', 'poverty_ratio', 'depression', 'yearly_alcohol', 'DBP', 'SBP']\n",
    "\n",
    "# columns not to transform\n",
    "all_cols = df.columns\n",
    "untransformed_cols = [x for x in all_cols if x in [*y_cols, t_col, z_col]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prune df\n",
    "#df.dropna(how='any', inplace=True)\n",
    "\n",
    "## transform df\n",
    "# apply scalers\n",
    "scaler = StandardScaler()\n",
    "numerical_transformation_cols = list(set(numerical_cols) - set(untransformed_cols))\n",
    "df[numerical_transformation_cols] = scaler.fit_transform(df[numerical_transformation_cols])\n",
    "\n",
    "# make z col binary (indicator of recommended max lux value during sleep)\n",
    "light_cutoff = 1 # nightly minute-mean summed light exposure\n",
    "df[z_col] = df[z_col].apply(lambda x: 1 if x <= light_cutoff else x)\n",
    "df[z_col] = df[z_col].apply(lambda x: 0 if x > light_cutoff else x)\n",
    "\n",
    "# one-hot encode multiclass categoricals\n",
    "multiclass_cols = df[categorical_cols].columns[df[categorical_cols].nunique() > 2].tolist() \n",
    "categorical_transformation_cols = list(set(multiclass_cols) - set(untransformed_cols))\n",
    "\n",
    "df = one_hot(df, categorical_transformation_cols)\n",
    "df.columns = df.columns.str.replace('.0', '')\n",
    "\n",
    "# get df as float\n",
    "df = df.astype(float)\n",
    "\n",
    "# get covariates\n",
    "x_cols = list(set(df.columns) - set([t_col, *y_cols, z_col]))\n",
    "\n",
    "# update lists of variable type\n",
    "all_cols = df.columns\n",
    "categorical_cols = [x for x in df.columns if any([x.startswith(y) for y in categorical_cols])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import statsmodels.api as sm\n",
    "\n",
    "class PropensityNet(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(PropensityNet, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, output_dim),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "def train_propensity_nn(X, T, n_classes, epochs=100, lr=0.001):\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    X_tensor = torch.tensor(X_scaled, dtype=torch.float32)\n",
    "    T_tensor = torch.tensor(T.values, dtype=torch.long)\n",
    "\n",
    "    model = PropensityNet(X.shape[1], n_classes)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    for _ in range(epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(X_tensor)\n",
    "        loss = criterion(logits, T_tensor)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        pi = model(X_tensor).numpy()\n",
    "    return pi, model, scaler\n",
    "\n",
    "\n",
    "class OutcomeNet(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(OutcomeNet, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "def train_outcome_nn(X, T, Y, n_classes, epochs=100, lr=0.001):\n",
    "    T_onehot = np.eye(n_classes)[T]\n",
    "    XT = np.concatenate([X, T_onehot], axis=1)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    XT_scaled = scaler.fit_transform(XT)\n",
    "\n",
    "    X_tensor = torch.tensor(XT_scaled, dtype=torch.float32)\n",
    "    Y_tensor = torch.tensor(Y.values, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "    model = OutcomeNet(XT.shape[1])\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.BCELoss()\n",
    "\n",
    "    for _ in range(epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(X_tensor)\n",
    "        loss = criterion(y_pred, Y_tensor)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    return model, scaler\n",
    "\n",
    "\n",
    "def aipw_estimate(X, T, Y, pi, mu_model, scaler, n_classes):\n",
    "    n = len(X)\n",
    "    tau_i = {}\n",
    "    mu = np.zeros((n, n_classes))\n",
    "\n",
    "    for k in range(n_classes):\n",
    "        T_k = np.eye(n_classes)[[k] * n]\n",
    "        XT_k = np.concatenate([X, T_k], axis=1)\n",
    "        XT_k_scaled = scaler.transform(XT_k)\n",
    "        X_tensor = torch.tensor(XT_k_scaled, dtype=torch.float32)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            preds = mu_model(X_tensor).numpy().flatten()\n",
    "\n",
    "        mu[:, k] = preds\n",
    "        treated = (T == k).astype(float).values\n",
    "        ipw_term = treated / pi[:, k] * (Y.values - mu[:, k])\n",
    "        tau_i[k] = mu[:, k] + ipw_term\n",
    "\n",
    "    results = {}\n",
    "    for i in range(n_classes):\n",
    "        for j in range(i + 1, n_classes):\n",
    "            diff = tau_i[j] - tau_i[i]\n",
    "            results[f\"{treatment_classes[j]} vs {treatment_classes[i]}\"] = {\n",
    "                \"tau\": diff.mean(),\n",
    "                \"variance\": diff.var()\n",
    "            }\n",
    "\n",
    "    return results, mu\n",
    "\n",
    "\n",
    "def run_weighted_logistic_regression(df, treatment_col, outcome_col, pi, treatment_classes):\n",
    "    # Use encoded treatment as input\n",
    "    T = df['T_encoded']\n",
    "    Y = df[outcome_col]\n",
    "\n",
    "    # Dummy encode treatment (reference = first class)\n",
    "    T_dummies = pd.get_dummies(T, prefix='T', drop_first=True)\n",
    "    X = sm.add_constant(T_dummies)\n",
    "    weights = 1.0 / pi[np.arange(len(pi)), T]\n",
    "\n",
    "    # Fit weighted logistic regression\n",
    "    model = sm.Logit(Y, X)\n",
    "    result = model.fit(weights=weights, disp=False)\n",
    "    X.columns = ['Intercept'] + [f\"{treatment_classes[int(c.split('_')[1])]} vs {treatment_classes[0]}\" for c in T_dummies.columns]\n",
    "    result.model.exog_names = X.columns  # rename for output readability\n",
    "\n",
    "    return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode treatment into integers\n",
    "df['T_encoded'], treatment_classes = pd.factorize(df['sleep_deprivation'])\n",
    "n_classes = len(treatment_classes)\n",
    "\n",
    "# Extract input arrays\n",
    "X = df[x_cols].values\n",
    "T = df['T_encoded']\n",
    "Y = df['HTN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi, prop_model, X_scaler = train_propensity_nn(X, T, n_classes)\n",
    "mu_model, XT_scaler = train_outcome_nn(X, T.values, Y, n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OutcomeNet(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(OutcomeNet, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "def train_outcome_nn(X, T, Y, n_classes, epochs=100, lr=0.001):\n",
    "    # One-hot encode T and concatenate with X\n",
    "    T_onehot = np.eye(n_classes)[T]\n",
    "    XT = np.concatenate([X, T_onehot], axis=1)\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    XT_scaled = scaler.fit_transform(XT)\n",
    "\n",
    "    X_tensor = torch.tensor(XT_scaled, dtype=torch.float32)\n",
    "    Y_tensor = torch.tensor(Y.values, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "    model = OutcomeNet(XT.shape[1])\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.BCELoss()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(X_tensor)\n",
    "        loss = criterion(y_pred, Y_tensor)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    return model, scaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aipw_estimate(X, T, Y, pi, mu_model, scaler, n_classes):\n",
    "    n = len(X)\n",
    "    tau_i = {}\n",
    "    mu = np.zeros((n, n_classes))\n",
    "\n",
    "    for k in range(n_classes):\n",
    "        # Construct counterfactual inputs for each treatment level\n",
    "        T_k = np.eye(n_classes)[[k] * n]\n",
    "        XT_k = np.concatenate([X, T_k], axis=1)\n",
    "        XT_k_scaled = scaler.transform(XT_k)\n",
    "\n",
    "        X_tensor = torch.tensor(XT_k_scaled, dtype=torch.float32)\n",
    "        with torch.no_grad():\n",
    "            preds = mu_model(X_tensor).numpy().flatten()\n",
    "\n",
    "        mu[:, k] = preds\n",
    "\n",
    "        # IPW adjustment\n",
    "        treated = (T == k).astype(float).values\n",
    "        ipw_term = treated / pi[:, k] * (Y.values - mu[:, k])\n",
    "        tau_i[k] = mu[:, k] + ipw_term\n",
    "\n",
    "    # Pairwise ATEs\n",
    "    results = {}\n",
    "    for i in range(n_classes):\n",
    "        for j in range(i + 1, n_classes):\n",
    "            diff = tau_i[j] - tau_i[i]\n",
    "            results[f\"{j} vs {i}\"] = {\n",
    "                \"tau\": diff.mean(),\n",
    "                \"variance\": diff.var()\n",
    "            }\n",
    "\n",
    "    return results, mu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "def run_weighted_logistic_regression(data, T_col, Y_col, pi, ref_group=0):\n",
    "    # Create dummy vars for treatment\n",
    "    T_dummies = pd.get_dummies(data[T_col], prefix='T', drop_first=True)\n",
    "    T_names = T_dummies.columns.tolist()\n",
    "    X = T_dummies\n",
    "    X = sm.add_constant(X)\n",
    "\n",
    "    # Inverse probability weights\n",
    "    weights = 1.0 / pi[np.arange(len(pi)), data[T_col].values]\n",
    "\n",
    "    # Logistic regression with IPW weights\n",
    "    model = sm.Logit(data[Y_col], X)\n",
    "    result = model.fit(weights=weights, disp=False)\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume: `data` is a pandas DataFrame with 'sleep_deprivation', 'hypertension', and covariates\n",
    "T = df['sleep_deprivation']\n",
    "Y = df['HTN']\n",
    "X = df[x_cols].values\n",
    "n_classes = T.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi, prop_model, X_scaler = train_propensity_nn(df[x_cols], T, n_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert treatment labels to integers if not already\n",
    "T_encoded, T_classes = pd.factorize(df['sleep_deprivation'])\n",
    "df['T_encoded'] = T_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_model, XT_scaler = train_outcome_nn(X, df['T_encoded'].values, Y, n_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "results, mu = aipw_estimate(X, T, Y, pi, mu_model, XT_scaler, n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1 vs 0': {'tau': -0.05634927771866319, 'variance': 3.6802786884034817},\n",
       " '2 vs 0': {'tau': -0.05312525559190594, 'variance': 9.778452056198859},\n",
       " '2 vs 1': {'tau': 0.0032240221267572137, 'variance': 6.693953511639587}}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Pandas data cast to numpy dtype of object. Check input data with np.asarray(data).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m logit_result \u001b[38;5;241m=\u001b[39m run_weighted_logistic_regression(df, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mT_encoded\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHTN\u001b[39m\u001b[38;5;124m'\u001b[39m, pi)\n",
      "Cell \u001b[0;32mIn[16], line 14\u001b[0m, in \u001b[0;36mrun_weighted_logistic_regression\u001b[0;34m(data, T_col, Y_col, pi, ref_group)\u001b[0m\n\u001b[1;32m     11\u001b[0m weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m/\u001b[39m pi[np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;28mlen\u001b[39m(pi)), data[T_col]\u001b[38;5;241m.\u001b[39mvalues]\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Logistic regression with IPW weights\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m model \u001b[38;5;241m=\u001b[39m sm\u001b[38;5;241m.\u001b[39mLogit(data[Y_col], X)\n\u001b[1;32m     15\u001b[0m result \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit(weights\u001b[38;5;241m=\u001b[39mweights, disp\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/scratch/users/melia/miniconda3/envs/cph200b_project3/lib/python3.11/site-packages/statsmodels/discrete/discrete_model.py:475\u001b[0m, in \u001b[0;36mBinaryModel.__init__\u001b[0;34m(self, endog, exog, offset, check_rank, **kwargs)\u001b[0m\n\u001b[1;32m    472\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, endog, exog, offset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, check_rank\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    473\u001b[0m     \u001b[38;5;66;03m# unconditional check, requires no extra kwargs added by subclasses\u001b[39;00m\n\u001b[1;32m    474\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_kwargs(kwargs)\n\u001b[0;32m--> 475\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(endog, exog, offset\u001b[38;5;241m=\u001b[39moffset, check_rank\u001b[38;5;241m=\u001b[39mcheck_rank,\n\u001b[1;32m    476\u001b[0m                      \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    477\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m, MultinomialModel):\n\u001b[1;32m    478\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39mall((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mendog \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m&\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mendog \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m)):\n",
      "File \u001b[0;32m/scratch/users/melia/miniconda3/envs/cph200b_project3/lib/python3.11/site-packages/statsmodels/discrete/discrete_model.py:185\u001b[0m, in \u001b[0;36mDiscreteModel.__init__\u001b[0;34m(self, endog, exog, check_rank, **kwargs)\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, endog, exog, check_rank\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_rank \u001b[38;5;241m=\u001b[39m check_rank\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(endog, exog, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    186\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraise_on_perfect_prediction \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m  \u001b[38;5;66;03m# keep for backwards compat\u001b[39;00m\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk_extra \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m/scratch/users/melia/miniconda3/envs/cph200b_project3/lib/python3.11/site-packages/statsmodels/base/model.py:270\u001b[0m, in \u001b[0;36mLikelihoodModel.__init__\u001b[0;34m(self, endog, exog, **kwargs)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, endog, exog\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 270\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(endog, exog, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    271\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minitialize()\n",
      "File \u001b[0;32m/scratch/users/melia/miniconda3/envs/cph200b_project3/lib/python3.11/site-packages/statsmodels/base/model.py:95\u001b[0m, in \u001b[0;36mModel.__init__\u001b[0;34m(self, endog, exog, **kwargs)\u001b[0m\n\u001b[1;32m     93\u001b[0m missing \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmissing\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnone\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     94\u001b[0m hasconst \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhasconst\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m---> 95\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle_data(endog, exog, missing, hasconst,\n\u001b[1;32m     96\u001b[0m                               \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk_constant \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mk_constant\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexog \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mexog\n",
      "File \u001b[0;32m/scratch/users/melia/miniconda3/envs/cph200b_project3/lib/python3.11/site-packages/statsmodels/base/model.py:135\u001b[0m, in \u001b[0;36mModel._handle_data\u001b[0;34m(self, endog, exog, missing, hasconst, **kwargs)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_handle_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, endog, exog, missing, hasconst, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 135\u001b[0m     data \u001b[38;5;241m=\u001b[39m handle_data(endog, exog, missing, hasconst, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;66;03m# kwargs arrays could have changed, easier to just attach here\u001b[39;00m\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m kwargs:\n",
      "File \u001b[0;32m/scratch/users/melia/miniconda3/envs/cph200b_project3/lib/python3.11/site-packages/statsmodels/base/data.py:675\u001b[0m, in \u001b[0;36mhandle_data\u001b[0;34m(endog, exog, missing, hasconst, **kwargs)\u001b[0m\n\u001b[1;32m    672\u001b[0m     exog \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(exog)\n\u001b[1;32m    674\u001b[0m klass \u001b[38;5;241m=\u001b[39m handle_data_class_factory(endog, exog)\n\u001b[0;32m--> 675\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m klass(endog, exog\u001b[38;5;241m=\u001b[39mexog, missing\u001b[38;5;241m=\u001b[39mmissing, hasconst\u001b[38;5;241m=\u001b[39mhasconst,\n\u001b[1;32m    676\u001b[0m              \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/scratch/users/melia/miniconda3/envs/cph200b_project3/lib/python3.11/site-packages/statsmodels/base/data.py:84\u001b[0m, in \u001b[0;36mModelData.__init__\u001b[0;34m(self, endog, exog, missing, hasconst, **kwargs)\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39morig_endog \u001b[38;5;241m=\u001b[39m endog\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39morig_exog \u001b[38;5;241m=\u001b[39m exog\n\u001b[0;32m---> 84\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mendog, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexog \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_endog_exog(endog, exog)\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconst_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk_constant \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m/scratch/users/melia/miniconda3/envs/cph200b_project3/lib/python3.11/site-packages/statsmodels/base/data.py:509\u001b[0m, in \u001b[0;36mPandasData._convert_endog_exog\u001b[0;34m(self, endog, exog)\u001b[0m\n\u001b[1;32m    507\u001b[0m exog \u001b[38;5;241m=\u001b[39m exog \u001b[38;5;28;01mif\u001b[39;00m exog \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m np\u001b[38;5;241m.\u001b[39masarray(exog)\n\u001b[1;32m    508\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m endog\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mobject\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m exog \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m exog\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mobject\u001b[39m:\n\u001b[0;32m--> 509\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPandas data cast to numpy dtype of object. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    510\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCheck input data with np.asarray(data).\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m_convert_endog_exog(endog, exog)\n",
      "\u001b[0;31mValueError\u001b[0m: Pandas data cast to numpy dtype of object. Check input data with np.asarray(data)."
     ]
    }
   ],
   "source": [
    "logit_result = run_weighted_logistic_regression(df, 'T_encoded', 'HTN', pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume: `data` is a pandas DataFrame with 'sleep_deprivation', 'hypertension', and covariates\n",
    "T = df['sleep_deprivation']\n",
    "Y = df['HTN']\n",
    "X = df[x_cols].values\n",
    "n_classes = T.nunique()\n",
    "\n",
    "# Step 1: Train propensity model\n",
    "pi, prop_model, X_scaler = train_propensity_nn(df[x_cols], T, n_classes)\n",
    "\n",
    "# Step 2: Train outcome model\n",
    "\n",
    "# Convert treatment labels to integers if not already\n",
    "T_encoded, T_classes = pd.factorize(df['sleep_deprivation'])\n",
    "df['T_encoded'] = T_encoded\n",
    "\n",
    "mu_model, XT_scaler = train_outcome_nn(X, df['T_encoded'].values, Y, n_classes)\n",
    "\n",
    "# Step 3: Compute AIPW estimates\n",
    "results, mu = aipw_estimate(X, T, Y, pi, mu_model, XT_scaler, n_classes)\n",
    "\n",
    "# Step 4: Weighted logistic regression\n",
    "logit_result = run_weighted_logistic_regression(data, 'sleep_deprivation', 'hypertension', pi)\n",
    "print(logit_result.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tS-learner score: 0.7884615384615384\n",
      "Feature Importance:  [1.49980180e-01 9.83740879e-03 7.73640218e-05 3.58134957e-03\n",
      " 4.62744444e-02 0.00000000e+00 7.57335057e-02 1.49470349e-03\n",
      " 6.91042464e-04 2.10812783e-03 0.00000000e+00 8.06301651e-04\n",
      " 3.08302280e-03 0.00000000e+00 8.68250147e-03 8.39488137e-04\n",
      " 2.74526261e-03 2.77988864e-03 5.18073429e-03 6.24120641e-04\n",
      " 1.90852194e-02 6.68592874e-04 0.00000000e+00 4.81749192e-04\n",
      " 8.51365573e-02 4.05790310e-02 4.75000392e-04 2.67353716e-02\n",
      " 1.98561028e-02 1.94659150e-03 1.33971060e-02 4.23662637e-01\n",
      " 3.19564570e-03 0.00000000e+00 2.05318625e-02 9.32497513e-03\n",
      " 3.19595378e-03 1.54241578e-03 5.98022938e-03 2.05386366e-03\n",
      " 7.63164900e-03]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The feature names should match those that were passed during fit.\nFeature names unseen at fit time:\n- _\n- a\n- d\n- e\n- i\n- ...\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[120], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m results, preds \u001b[38;5;241m=\u001b[39m aipw_estimator(df, \n\u001b[1;32m      2\u001b[0m                 treatment_var\u001b[38;5;241m=\u001b[39mt_col, \n\u001b[1;32m      3\u001b[0m                 outcome_var\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHTN\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[1;32m      4\u001b[0m                 covariates\u001b[38;5;241m=\u001b[39mx_cols,\n\u001b[1;32m      5\u001b[0m                 continuous_outcome\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m/scratch/users/melia/CPH200B_25/cph-200b-project3/src/estimators.py:222\u001b[0m, in \u001b[0;36maipw_estimator\u001b[0;34m(data, treatment_var, outcome_var, covariates, continuous_outcome, **kwargs)\u001b[0m\n\u001b[1;32m      0\u001b[0m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "File \u001b[0;32m/scratch/users/melia/miniconda3/envs/cph200b_project3/lib/python3.11/site-packages/sklearn/ensemble/_gb.py:1667\u001b[0m, in \u001b[0;36mGradientBoostingClassifier.predict_proba\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m   1646\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict_proba\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[1;32m   1647\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Predict class probabilities for X.\u001b[39;00m\n\u001b[1;32m   1648\u001b[0m \n\u001b[1;32m   1649\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1665\u001b[0m \u001b[38;5;124;03m        If the ``loss`` does not support probabilities.\u001b[39;00m\n\u001b[1;32m   1666\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1667\u001b[0m     raw_predictions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecision_function(X)\n\u001b[1;32m   1668\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_loss\u001b[38;5;241m.\u001b[39mpredict_proba(raw_predictions)\n",
      "File \u001b[0;32m/scratch/users/melia/miniconda3/envs/cph200b_project3/lib/python3.11/site-packages/sklearn/ensemble/_gb.py:1565\u001b[0m, in \u001b[0;36mGradientBoostingClassifier.decision_function\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m   1546\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecision_function\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[1;32m   1547\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute the decision function of ``X``.\u001b[39;00m\n\u001b[1;32m   1548\u001b[0m \n\u001b[1;32m   1549\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1563\u001b[0m \u001b[38;5;124;03m        array of shape (n_samples,).\u001b[39;00m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1565\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_data(\n\u001b[1;32m   1566\u001b[0m         X, dtype\u001b[38;5;241m=\u001b[39mDTYPE, order\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC\u001b[39m\u001b[38;5;124m\"\u001b[39m, accept_sparse\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsr\u001b[39m\u001b[38;5;124m\"\u001b[39m, reset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1567\u001b[0m     )\n\u001b[1;32m   1568\u001b[0m     raw_predictions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raw_predict(X)\n\u001b[1;32m   1569\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m raw_predictions\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m/scratch/users/melia/miniconda3/envs/cph200b_project3/lib/python3.11/site-packages/sklearn/base.py:608\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_validate_data\u001b[39m(\n\u001b[1;32m    538\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    539\u001b[0m     X\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mno_validation\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    544\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params,\n\u001b[1;32m    545\u001b[0m ):\n\u001b[1;32m    546\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Validate input data and set or check the `n_features_in_` attribute.\u001b[39;00m\n\u001b[1;32m    547\u001b[0m \n\u001b[1;32m    548\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    606\u001b[0m \u001b[38;5;124;03m        validated.\u001b[39;00m\n\u001b[1;32m    607\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 608\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_feature_names(X, reset\u001b[38;5;241m=\u001b[39mreset)\n\u001b[1;32m    610\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_tags()[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequires_y\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m    611\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    612\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m estimator \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    613\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequires y to be passed, but the target y is None.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    614\u001b[0m         )\n",
      "File \u001b[0;32m/scratch/users/melia/miniconda3/envs/cph200b_project3/lib/python3.11/site-packages/sklearn/base.py:535\u001b[0m, in \u001b[0;36mBaseEstimator._check_feature_names\u001b[0;34m(self, X, reset)\u001b[0m\n\u001b[1;32m    530\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m missing_names \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m unexpected_names:\n\u001b[1;32m    531\u001b[0m     message \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    532\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFeature names must be in the same order as they were in fit.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    533\u001b[0m     )\n\u001b[0;32m--> 535\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(message)\n",
      "\u001b[0;31mValueError\u001b[0m: The feature names should match those that were passed during fit.\nFeature names unseen at fit time:\n- _\n- a\n- d\n- e\n- i\n- ...\n"
     ]
    }
   ],
   "source": [
    "results, preds = aipw_estimator(df, \n",
    "                treatment_var=t_col, \n",
    "                outcome_var='HTN', \n",
    "                covariates=x_cols,\n",
    "                continuous_outcome=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimators = [unadjusted_DM_estimator, ipw_estimator, t_learner, s_learner, x_learner, aipw_estimator]\n",
    "estimator_names = estimator_names = [x.__name__ for x in estimators]\n",
    "pairs = [' vs '.join(x[::-1]) for x in itertools.combinations(df[t_col].unique().astype(int).astype(str), 2)]\n",
    "index = pd.MultiIndex.from_product([estimator_names, pairs, y_cols], names=['estimator', 'pair','outcome'])\n",
    "tau_results = pd.DataFrame(index=index, columns=['tau'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1 vs 0': {'tau': -0.027609923703864274, 'variance': 0.6068964341725066},\n",
       " '2 vs 0': {'tau': -0.015327715135803135, 'variance': 1.0969490768404648},\n",
       " '2 vs 1': {'tau': 0.01228220856806113, 'variance': 0.974821312483853}}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.79683972, 0.7463666 , 0.7463666 ],\n",
       "       [0.89624143, 0.89624143, 0.89624143],\n",
       "       [0.70306437, 0.70306437, 0.70306437],\n",
       "       ...,\n",
       "       [0.40388258, 0.40388258, 0.40388258],\n",
       "       [0.85581417, 0.85581417, 0.85581417],\n",
       "       [0.79797705, 0.79797705, 0.79797705]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimating effect of sleep_deprivation on HTN using unadjusted_DM_estimator...\n",
      "Estimating effect of sleep_deprivation on DBP using unadjusted_DM_estimator...\n",
      "Estimating effect of sleep_deprivation on SBP using unadjusted_DM_estimator...\n",
      "Estimating effect of sleep_deprivation on HTN using ipw_estimator...\n",
      "Estimating effect of sleep_deprivation on DBP using ipw_estimator...\n",
      "Estimating effect of sleep_deprivation on SBP using ipw_estimator...\n",
      "Estimating effect of sleep_deprivation on HTN using t_learner...\n",
      "Estimating effect of sleep_deprivation on DBP using t_learner...\n",
      "Estimating effect of sleep_deprivation on SBP using t_learner...\n",
      "Estimating effect of sleep_deprivation on HTN using s_learner...\n",
      "Estimating effect of sleep_deprivation on DBP using s_learner...\n",
      "Estimating effect of sleep_deprivation on SBP using s_learner...\n",
      "Estimating effect of sleep_deprivation on HTN using x_learner...\n",
      "Estimating effect of sleep_deprivation on DBP using x_learner...\n",
      "Estimating effect of sleep_deprivation on SBP using x_learner...\n",
      "Estimating effect of sleep_deprivation on HTN using aipw_estimator...\n",
      "\tS-learner score: 0.7841444270015698\n",
      "Estimating effect of sleep_deprivation on DBP using aipw_estimator...\n",
      "\tS-learner score: 0.2870246924644897\n",
      "Estimating effect of sleep_deprivation on SBP using aipw_estimator...\n",
      "\tS-learner score: 0.3395893618376288\n"
     ]
    }
   ],
   "source": [
    "for tau_estimator, outcome in itertools.product(estimators, y_cols):\n",
    "    estimator_name = tau_estimator.__name__\n",
    "    print(f'Estimating effect of {t_col} on {outcome} using {estimator_name}...')\n",
    "    continuous_outcome = False if outcome == 'HTN' else True\n",
    "    results = tau_estimator(df, \n",
    "                            treatment_var=t_col, \n",
    "                            outcome_var=outcome, \n",
    "                            covariates=x_cols,\n",
    "                            continuous_outcome=continuous_outcome)\n",
    "    \n",
    "    for pair in results.keys():\n",
    "        tau = results[pair]['tau']\n",
    "        tau_results.loc[(estimator_name, pair, outcome)] = tau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "tau_results.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>estimator</th>\n",
       "      <th>pair</th>\n",
       "      <th>outcome</th>\n",
       "      <th>tau</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>unadjusted_DM_estimator</td>\n",
       "      <td>1 vs 0</td>\n",
       "      <td>HTN</td>\n",
       "      <td>-0.078861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>unadjusted_DM_estimator</td>\n",
       "      <td>2 vs 0</td>\n",
       "      <td>HTN</td>\n",
       "      <td>-0.015747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>unadjusted_DM_estimator</td>\n",
       "      <td>2 vs 1</td>\n",
       "      <td>HTN</td>\n",
       "      <td>0.063114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ipw_estimator</td>\n",
       "      <td>1 vs 0</td>\n",
       "      <td>HTN</td>\n",
       "      <td>0.005025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>ipw_estimator</td>\n",
       "      <td>2 vs 0</td>\n",
       "      <td>HTN</td>\n",
       "      <td>-0.139962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ipw_estimator</td>\n",
       "      <td>2 vs 1</td>\n",
       "      <td>HTN</td>\n",
       "      <td>-0.144987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>t_learner</td>\n",
       "      <td>1 vs 0</td>\n",
       "      <td>HTN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>t_learner</td>\n",
       "      <td>2 vs 0</td>\n",
       "      <td>HTN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>t_learner</td>\n",
       "      <td>2 vs 1</td>\n",
       "      <td>HTN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>s_learner</td>\n",
       "      <td>1 vs 0</td>\n",
       "      <td>HTN</td>\n",
       "      <td>-0.010694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>s_learner</td>\n",
       "      <td>2 vs 0</td>\n",
       "      <td>HTN</td>\n",
       "      <td>-0.005026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>s_learner</td>\n",
       "      <td>2 vs 1</td>\n",
       "      <td>HTN</td>\n",
       "      <td>0.005668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>x_learner</td>\n",
       "      <td>1 vs 0</td>\n",
       "      <td>HTN</td>\n",
       "      <td>-0.021806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>x_learner</td>\n",
       "      <td>2 vs 0</td>\n",
       "      <td>HTN</td>\n",
       "      <td>-0.002213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>x_learner</td>\n",
       "      <td>2 vs 1</td>\n",
       "      <td>HTN</td>\n",
       "      <td>0.019593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>aipw_estimator</td>\n",
       "      <td>1 vs 0</td>\n",
       "      <td>HTN</td>\n",
       "      <td>-0.02756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>aipw_estimator</td>\n",
       "      <td>2 vs 0</td>\n",
       "      <td>HTN</td>\n",
       "      <td>-0.015292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>aipw_estimator</td>\n",
       "      <td>2 vs 1</td>\n",
       "      <td>HTN</td>\n",
       "      <td>0.012268</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  estimator    pair outcome       tau\n",
       "0   unadjusted_DM_estimator  1 vs 0     HTN -0.078861\n",
       "3   unadjusted_DM_estimator  2 vs 0     HTN -0.015747\n",
       "6   unadjusted_DM_estimator  2 vs 1     HTN  0.063114\n",
       "9             ipw_estimator  1 vs 0     HTN  0.005025\n",
       "12            ipw_estimator  2 vs 0     HTN -0.139962\n",
       "15            ipw_estimator  2 vs 1     HTN -0.144987\n",
       "18                t_learner  1 vs 0     HTN       0.0\n",
       "21                t_learner  2 vs 0     HTN       0.0\n",
       "24                t_learner  2 vs 1     HTN       0.0\n",
       "27                s_learner  1 vs 0     HTN -0.010694\n",
       "30                s_learner  2 vs 0     HTN -0.005026\n",
       "33                s_learner  2 vs 1     HTN  0.005668\n",
       "36                x_learner  1 vs 0     HTN -0.021806\n",
       "39                x_learner  2 vs 0     HTN -0.002213\n",
       "42                x_learner  2 vs 1     HTN  0.019593\n",
       "45           aipw_estimator  1 vs 0     HTN  -0.02756\n",
       "48           aipw_estimator  2 vs 0     HTN -0.015292\n",
       "51           aipw_estimator  2 vs 1     HTN  0.012268"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tau_results[tau_results['outcome']=='HTN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cph200b_project3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
